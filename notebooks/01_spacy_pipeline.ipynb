{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "\n",
    "\n",
    "In the last notebook, we saw a simple example of using cycontext to extract contextual information about a clinical entity. In this notebook we'll show how to integrate cycontext into a spaCy pipeline for scalable clinical text processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.pipeline import EntityRuler\n",
    "\n",
    "from cycontext import ConTextComponent, ConTextItem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy Pipelines\n",
    "A spaCy model process texts through a sequential process called a \"pipeline\". Each part of a pipeline is called a **component** and handles a different part of text processing. This allows our models to be modular and flexible. \n",
    "\n",
    "By default, a processing pipeline looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"200\" viewBox=\"0 0 923 200\" width=\"923\" xmlns=\"http://www.w3.org/2000/svg\">\n",
       "    <style>\n",
       "        .svg__pipeline__text { fill: #1a1e23; font: 20px Arial, sans-serif }\n",
       "        .svg__pipeline__text-small { fill: #1a1e23; font: bold 18px Arial, sans-serif }\n",
       "        .svg__pipeline__text-code { fill: #1a1e23; font: 600 16px Menlo, Monaco, Consolas, &quot;Liberation Mono&quot;, &quot;Courier New&quot;, monospace }\n",
       "    </style>\n",
       "    <rect fill=\"none\" height=\"127\" rx=\"19.1\" ry=\"19.1\" stroke=\"#09a3d5\" stroke-dasharray=\"3 6\" stroke-width=\"3\" width=\"601\" x=\"159\" y=\"21\"/>\n",
       "    <path d=\"M801 55h120v60H801z\" fill=\"#e1d5e7\" stroke=\"#9673a6\" stroke-width=\"2\"/>\n",
       "    <text class=\"svg__pipeline__text\" dy=\"0.75em\" height=\"19\" transform=\"translate(846.5 75.5)\" width=\"28\">Doc</text>\n",
       "    <path d=\"M121.2 84.7h29.4\" fill=\"none\" stroke=\"#999\" stroke-miterlimit=\"10\" stroke-width=\"2\"/>\n",
       "    <path d=\"M156.6 84.7l-8 4 2-4-2-4z\" fill=\"#999\" stroke=\"#999\" stroke-miterlimit=\"10\" stroke-width=\"2\"/>\n",
       "    <path d=\"M1 55h120v60H1z\" fill=\"#f5f5f5\" stroke=\"#999\" stroke-width=\"2\"/>\n",
       "    <text class=\"svg__pipeline__text\" dy=\"0.85em\" height=\"22\" transform=\"translate(43.5 73.5)\" width=\"34\">Text</text>\n",
       "    <path d=\"M760 84.7h33\" fill=\"none\" stroke=\"#999\" stroke-miterlimit=\"10\" stroke-width=\"2\"/>\n",
       "    <path d=\"M799 84.7l-8 4 2-4-2-4z\" fill=\"#999\" stroke=\"#999\" stroke-miterlimit=\"10\" stroke-width=\"2\"/>\n",
       "    <rect fill=\"#dae8fc\" height=\"39\" rx=\"5.8\" ry=\"5.8\" stroke=\"#09a3d5\" stroke-width=\"2\" width=\"75\" x=\"422\" y=\"1\"/>\n",
       "    <text class=\"svg__pipeline__text-code\" dx=\"0.1em\" dy=\"0.8em\" height=\"17\" transform=\"translate(444.5 11.5)\" width=\"29\">nlp</text>\n",
       "    <path d=\"M176 58h103.3L296 88l-16.8 30H176l16.8-30z\" fill=\"#f8cecc\" stroke=\"#b85450\" stroke-miterlimit=\"10\" stroke-width=\"2\"/>\n",
       "    <text class=\"svg__pipeline__text-small\" dx=\"-0.25em\" dy=\"0.75em\" height=\"14\" transform=\"translate(206.5 80.5)\" width=\"58\">tokenizer</text>\n",
       "    <path d=\"M314 58h103.3L434 88l-16.8 30H314l16.8-30z\" fill=\"#ffe6cc\" stroke=\"#d79b00\" stroke-miterlimit=\"10\" stroke-width=\"2\"/>\n",
       "    <text class=\"svg__pipeline__text-small\" dx=\"8\" dy=\"0.75em\" height=\"14\" transform=\"translate(342.5 80.5)\" width=\"62\">tagger</text>\n",
       "    <path d=\"M296.5 88.2h24.7\" fill=\"none\" stroke=\"#999\" stroke-miterlimit=\"10\" stroke-width=\"2\"/>\n",
       "    <path d=\"M327.2 88.2l-8 4 2-4-2-4z\" fill=\"#999\" stroke=\"#999\" stroke-miterlimit=\"10\" stroke-width=\"2\"/>\n",
       "    <path d=\"M416 58h103.3L536 88l-16.8 30H416l16.8-30z\" fill=\"#ffe6cc\" stroke=\"#d79b00\" stroke-miterlimit=\"10\" stroke-width=\"2\"/>\n",
       "    <text class=\"svg__pipeline__text-small\" dx=\"-0.25em\" dy=\"0.75em\" height=\"14\" transform=\"translate(455.5 80.5)\" width=\"40\">parser</text>\n",
       "    <path d=\"M519 58h103.3L639 88l-16.8 30H519l16.8-30z\" fill=\"#ffe6cc\" stroke=\"#d79b00\" stroke-miterlimit=\"10\" stroke-width=\"2\"/>\n",
       "    <text class=\"svg__pipeline__text-small\" dx=\"8\" dy=\"0.75em\" height=\"14\" transform=\"translate(558.5 80.5)\" width=\"40\">ner</text>\n",
       "    <path d=\"M622 58h103.3L742 88l-16.8 30H622l16.8-30z\" fill=\"#ffe6cc\" stroke=\"#d79b00\" stroke-miterlimit=\"10\" stroke-width=\"2\"/>\n",
       "    <text class=\"svg__pipeline__text-small\" dx=\"8\" dy=\"0.75em\" height=\"14\" transform=\"translate(671.5 80.5)\" width=\"20\">...</text>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "SVG(\"./spacy_pipeline.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most powerful features of spaCy is that we can customize a pipeline to define our own processing on a text. `cycontext` is designed to be used as a spaCy component and to fit within a processing pipeline. In this notebook, we'll walk through how to add ConText to a pipeline for processing multiple clinical documents. In this notebook, we'll define several texts with multiple targets and modifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"There is no evidence of pneumonia.\",\n",
    "    \"rule out pna\",\n",
    "    \"HISTORY OF PE.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model\n",
    "We start by loading a standard spaCy model which comes with a standard processing pipeline. However, we'll remove the standard `ner` model since we'll be defining our own entity classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=\"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tagger', <spacy.pipeline.pipes.Tagger at 0x10faa3f98>),\n",
       " ('parser', <spacy.pipeline.pipes.DependencyParser at 0x10fc417c8>)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Concept Extraction\n",
    "As cycontext is modular, it does not handle extracting target concepts. These concepts should be extracted earlier in a spaCy pipeline and stored in `doc.ents`. There are various methods for extracting these target concepts. One way is through [rule-based matching](https://spacy.io/usage/rule-based-matching), such as spaCy's `EntityRuler` class. \n",
    "\n",
    "The sentences above have the following target entities:\n",
    "- \"pneumonia\"\n",
    "- \"pna\"\n",
    "- \"PE\"\n",
    "\n",
    "We instantiate an EntityRuler class and then define patterns to match these concepts in the texts. In the spaCy API, we do this by defining a list of dictionaries which define the label and pattern for each concept. \n",
    "\n",
    "For more detailed information, see spaCy's documentation on rule-based matching: https://spacy.io/usage/rule-based-matching. \n",
    "\n",
    "In this example, we'll assign a label of **\"CONDITION\"** to each entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruler = EntityRuler(nlp, overwrite_ents=True, phrase_matcher_attr=\"LOWER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_patterns = [\n",
    "    {\n",
    "        'label': 'CONDITION',\n",
    "        'pattern': \"pneumonia\"\n",
    "    },\n",
    "    {\n",
    "        'label': 'CONDITION',\n",
    "        'pattern': \"pna\"\n",
    "    },\n",
    "    {\n",
    "        'label': 'CONDITION',\n",
    "        'pattern': \"PE\"\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then add these to the ruler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruler.add_patterns(target_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then add this to our pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(ruler, last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tagger', <spacy.pipeline.pipes.Tagger at 0x10faa3f98>),\n",
       " ('parser', <spacy.pipeline.pipes.DependencyParser at 0x10fc417c8>),\n",
       " ('entity_ruler', <spacy.pipeline.entityruler.EntityRuler at 0x10f9b5c88>)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now process each of the texts and find our extracted targets in the `ents` attribute. We process a list of texts by calling `list(nlp.pipe(texts))`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = list(nlp.pipe(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(pneumonia,)\n",
      "(pna,)\n",
      "(PE,)\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    print(doc.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConText\n",
    "Once we have extracted target concepts in our pipeline, we define the modifiers. In these examples, there are 3 modifiers to be extracted:\n",
    "- **\"no evidence of\"**: this signifies negation. We call this **DEFINITE_NEGATED_EXISTENCE**\n",
    "- **\"rule out\"**: this signifies that the purpose of an exam is to check whether a patient has a condition. We call this **INDICATION**\n",
    "- **\"PMH\"**: this signifies \"past medical history\" and that a condition is not current. We call this \"HISTORICAL\"\n",
    "\n",
    "We define these and store them in a list of ConTextItems:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then instantiate our ConTextComponent with the default rules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = ConTextComponent(nlp, rules=\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item_data = [\n",
    "#     ConTextItem(\"no evidence of\", category=\"DEFINITE_NEGATED_EXISTENCE\", rule=\"forward\"),\n",
    "#     ConTextItem(\"rule out\", category=\"INDICATION\", rule=\"forward\"),\n",
    "#     ConTextItem(\"Past Medical History\", category=\"HISTORICAL\", rule=\"forward\")\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context.add(item_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FAMILY',\n",
       " 'HISTORICAL',\n",
       " 'HYPOTHETICAL',\n",
       " 'NEGATED_EXISTENCE',\n",
       " 'POSSIBLE_EXISTENCE'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context.categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last notebook, we called context directly on a doc: `context(doc)`. We'll now instead add it to the NLP pipeline so it will process each of the documents. Because it follows target concept extraction, we can specify its place in a pipeline by setting the `after` argument to `\"entity_ruler\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(context, after=\"entity_ruler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we process our texts we can see the ConText information which we extracted manually in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = list(nlp.pipe(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docs:\n",
    "    for target, modifier in doc._.context_graph.edges:\n",
    "        print(\"[{0}] is modified by [{1}]\".format(target, modifier))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using the visualization module, colors can be optionally supplied to the `colors` argument of `visualize_ent`. If left to `None`, a color cycle will be generated using the default matplotlib colors. However, we can also define them manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cycontext import viz\n",
    "colors = {\"NEGATED_EXISTENCE\": \"#34b1eb\",\n",
    "                          \"POSSIBLE_EXISTENCE\": \"yellow\",\n",
    "                           \"HISTORICAL\": \"#ddb8f5\",\n",
    "                          \"CONDITION\": \"orange\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.visualize_ent(docs[0], \n",
    "                  colors=colors\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.visualize_ent(docs[1], \n",
    "                  colors=colors\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cycontext import viz\n",
    "viz.visualize_ent(docs[2], \n",
    "                  colors=colors,\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.visualize_dep(docs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
